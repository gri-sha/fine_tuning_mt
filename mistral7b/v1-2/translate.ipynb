{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa177eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the project root directory and add it to the system path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64bfa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from util import generate_simple_eval_prompts, initialize_dfs\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import set_seed, AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e1ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1212\n",
    "\n",
    "# repeat the same splits with the same seed\n",
    "test_split = 0.1  # 10% of the all\n",
    "valid_split = 0.05  # 5% of the training set\n",
    "\n",
    "max_new_tokens=40\n",
    "batch_size = 16\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_checkpoint = os.path.expanduser(\"~/finetuning_mistral7b_v2/checkpoint-90\")\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface/\")\n",
    "translations_path = os.path.join(project_root, \"translations/mistral7b_v2_translations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a649fb-e756-465d-a57b-c72c82f85250",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcff30e-73c9-46fd-a15a-e7eb1dd8540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, df_test = initialize_dfs(test=test_split)\n",
    "s0, r0, p0 = generate_simple_eval_prompts(df_test, shots=0)\n",
    "s1, r1, p1 = generate_simple_eval_prompts(df_test, shots=1, fuzzy=True)\n",
    "sources = s0 + s1\n",
    "references = r0 + r1\n",
    "prompts = p0 + p1\n",
    "dataset = Dataset.from_dict({\"sources\": sources, \"references\": references, \"prompts\": prompts})\n",
    "pprint(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e44d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "peftconfig = PeftConfig.from_pretrained(model_checkpoint)\n",
    "\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    peftconfig.base_model_name_or_path, device_map=\"auto\", cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=cache_dir,\n",
    "    add_bos_token=True,\n",
    "    add_eos_token=False,  # always False for inference\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = PeftModel.from_pretrained(model_base, model_checkpoint)\n",
    "print(\"Peft model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8086a573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_responses(prompts, model):\n",
    "    encoded = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **encoded,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            min_new_tokens=1,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c0244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = []\n",
    "for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "    batch_prompts = dataset[\"prompts\"][i:i + batch_size]\n",
    "    responses = generate_batch_responses(batch_prompts, model)\n",
    "    cleaned = [r.replace(p, \"\") for p, r in zip(batch_prompts, responses)]\n",
    "    translations.extend(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a95386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory\n",
    "folder = os.path.dirname(translations_path)\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# Save results\n",
    "translations_df = pd.DataFrame({\n",
    "    \"sources\": sources,\n",
    "    \"references\": references,\n",
    "    \"translations\": translations\n",
    "})\n",
    "translations_df.to_csv(translations_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
