{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64bfa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/grisha/Developer/fine_tuning_mt/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from util import generate_eval_prompts, initialize_dfs\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import set_seed, AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from pprint import pprint\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe4e1ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1212\n",
    "\n",
    "# repeat the same splits with the same seed\n",
    "test_split = 0.1  # 10% of the all\n",
    "valid_split = 0.05  # 5% of the training set\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_checkpoint = os.path.expanduser(\"~/finetuning_mistral7b_v1/checkpoint-89\")\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface/\")\n",
    "translations_path = \"translations/mistral7b_translations.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9a649fb-e756-465d-a57b-c72c82f85250",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcff30e-73c9-46fd-a15a-e7eb1dd8540c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing dataframe...\n",
      "Dataframe loaded.\n",
      "Split at index 10119.\n",
      "['Chapter 34',\n",
      " 'His fingers tweak one of my nipples, and I moan into yet another kiss.',\n",
      " 'God, that felt so fucking good.',\n",
      " \"But he wasn't good at that kind of thing.\",\n",
      " \"I don't only mean the millions of songs in the digital archive, someone in \"\n",
      " \"the station's history had hoarded vinyl like it was going out of style.\"]\n",
      "['English: Chapter 34\\nFrench: ',\n",
      " 'English: His fingers tweak one of my nipples, and I moan into yet another '\n",
      " 'kiss.\\n'\n",
      " 'French: ',\n",
      " 'English: God, that felt so fucking good.\\nFrench: ',\n",
      " \"English: But he wasn't good at that kind of thing.\\nFrench: \",\n",
      " \"English: I don't only mean the millions of songs in the digital archive, \"\n",
      " \"someone in the station's history had hoarded vinyl like it was going out of \"\n",
      " 'style.\\n'\n",
      " 'French: ']\n",
      "Dataset({\n",
      "    features: ['prompts', 'references'],\n",
      "    num_rows: 2250\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "_, df_test = initialize_dfs(test=test_split)\n",
    "r0, p0 = generate_eval_prompts(df_test, shots=0)\n",
    "r1, p1 = generate_eval_prompts(df_test, shots=1, fuzzy=True)\n",
    "references = r0 + r1\n",
    "prompts = p0 + p1\n",
    "dataset = Dataset.from_dict({\"prompts\": prompts, \"references\": references})\n",
    "pprint(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e44d769",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at '/Users/grisha/finetuning_mistral7b_v1/checkpoint-89'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Developer/fine_tuning_mt/.venv/lib/python3.9/site-packages/peft/config.py:200\u001b[0m, in \u001b[0;36mPeftConfigMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m     config_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhf_hub_download_kwargs\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Developer/fine_tuning_mt/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Developer/fine_tuning_mt/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/Users/grisha/finetuning_mistral7b_v1/checkpoint-89'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m peftconfig \u001b[38;5;241m=\u001b[39m \u001b[43mPeftConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model_base \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      4\u001b[0m     peftconfig\u001b[38;5;241m.\u001b[39mbase_model_name_or_path, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      8\u001b[0m     model_name,\n\u001b[1;32m      9\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m     10\u001b[0m     add_bos_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m     add_eos_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# always False for inference\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m~/Developer/fine_tuning_mt/.venv/lib/python3.9/site-packages/peft/config.py:204\u001b[0m, in \u001b[0;36mPeftConfigMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m         config_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    201\u001b[0m             pretrained_model_name_or_path, CONFIG_NAME, subfolder\u001b[38;5;241m=\u001b[39msubfolder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_hub_download_kwargs\n\u001b[1;32m    202\u001b[0m         )\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    206\u001b[0m loaded_attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_json_file(config_file)\n\u001b[1;32m    207\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclass_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloaded_attributes}\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at '/Users/grisha/finetuning_mistral7b_v1/checkpoint-89'"
     ]
    }
   ],
   "source": [
    "peftconfig = PeftConfig.from_pretrained(model_checkpoint)\n",
    "\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    peftconfig.base_model_name_or_path, device_map=\"auto\", cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=cache_dir,\n",
    "    add_bos_token=True,\n",
    "    add_eos_token=False,  # always False for inference\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model_base, model_checkpoint)\n",
    "print(\"Peft model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8086a573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_responses(prompts, model):\n",
    "    encoded = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **encoded,\n",
    "            max_new_tokens=20,\n",
    "            min_new_tokens=1,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c0244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = []\n",
    "for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "    batch_prompts = dataset[\"prompts\"][i:i + batch_size]\n",
    "    responses = generate_batch_responses(batch_prompts, model)\n",
    "    cleaned = [r.replace(p, \"\") for p, r in zip(batch_prompts, responses)]\n",
    "    translations.extend(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a95386",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations_df = pd.DataFrame({\n",
    "    \"reference\": references,\n",
    "    \"response\": translations\n",
    "})\n",
    "translations_df.to_csv(translations_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
